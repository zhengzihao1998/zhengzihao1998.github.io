
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="笔记">
      
      
        <meta name="author" content="zhengzihao">
      
      
        <link rel="canonical" href="https://zhengzihao1998.github.io./blog/2024/09/16/%E5%9F%BA%E4%BA%8E%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%A6%BB%E6%95%A3%E6%97%B6%E9%97%B4%E5%9D%87%E5%80%BC%E6%96%B9%E5%B7%AE%E7%AD%96%E7%95%A5/">
      
      
        <link rel="prev" href="../../13/%E7%89%B9%E5%BE%81%E6%8E%92%E5%BA%8F%E5%9B%A0%E5%AD%90%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">
      
      
        <link rel="next" href="../../19/%E9%9D%99%E6%80%81%E9%A1%B5%E9%9D%A2%E7%9B%B8%E5%85%B3/">
      
      
      <link rel="icon" href="../../../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.34">
    
    
      
        <title>基于强化学习的离散时间均值方差策略 - 个人笔记</title>
      
    
    
      <link rel="stylesheet" href="../../../../../assets/stylesheets/main.35f28582.min.css">
      
      

  
  
  
  
  <style>:root{--md-annotation-icon:url('data:image/svg+xml;charset=utf-8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M22 12a10 10 0 0 1-10 10A10 10 0 0 1 2 12 10 10 0 0 1 12 2a10 10 0 0 1 10 10M6 13h8l-3.5 3.5 1.42 1.42L17.84 12l-5.92-5.92L10.5 7.5 14 11H6z"/></svg>');}</style>


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#discrete-time-mean-variance-strategy-based-on-reinforcement-learning" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../../.." title="个人笔记" class="md-header__button md-logo" aria-label="个人笔记" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            个人笔记
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              基于强化学习的离散时间均值方差策略
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
                
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" hidden>
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../../.." title="个人笔记" class="md-nav__button md-logo" aria-label="个人笔记" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    个人笔记
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    主页
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
          
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../../" class="md-nav__link ">
              
  
  <span class="md-ellipsis">
    笔记
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_2" id="__nav_2_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            笔记
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    
    
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_2" >
        
          
          <label class="md-nav__link" for="__nav_2_2" id="__nav_2_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Archive
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_2">
            <span class="md-nav__icon md-icon"></span>
            Archive
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../archive/2024/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2024
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
        
      
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_3" >
        
          
          <label class="md-nav__link" for="__nav_2_3" id="__nav_2_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Categories
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_3">
            <span class="md-nav__icon md-icon"></span>
            Categories
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/deep-learning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Deep Learning
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/git/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Git
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
    
  
  
    <li class="md-nav__item">
      <a href="../../../../category/reinforcement-learning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Reinforcement Learning
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../about/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    关于我
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
                
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#2" class="md-nav__link">
    <span class="md-ellipsis">
      2 离散时间探索性的均值方差问题
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2 离散时间探索性的均值方差问题">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21" class="md-nav__link">
    <span class="md-ellipsis">
      2.1 经典离散时间均值方差问题
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22" class="md-nav__link">
    <span class="md-ellipsis">
      2.2 离散时间探索性的均值方差问题
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3" class="md-nav__link">
    <span class="md-ellipsis">
      3. 离散时间算法
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3. 离散时间算法">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31" class="md-nav__link">
    <span class="md-ellipsis">
      3.1 策略提升定理和策略收敛
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
  <div class="md-content md-content--post" data-md-component="content">
    <div class="md-sidebar md-sidebar--post" data-md-component="sidebar" data-md-type="navigation">
      <div class="md-sidebar__scrollwrap">
        <div class="md-sidebar__inner md-post">
          <nav class="md-nav md-nav--primary">
            <div class="md-post__back">
              <div class="md-nav__title md-nav__container">
                <a href="../../../../" class="md-nav__link">
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
                  <span class="md-ellipsis">
                    Back to index
                  </span>
                </a>
              </div>
            </div>
            
              <div class="md-post__authors md-typeset">
                
                  <div class="md-profile md-post__profile">
                    <span class="md-author md-author--long">
                      <img src="https://avatars.githubusercontent.com/u/158252341?v=4" alt="zhengzihao">
                    </span>
                    <span class="md-profile__description">
                      <strong>
                        
                          <a href="https://github.com/zhengzihao1998">zhengzihao</a>
                        
                      </strong>
                      <br>
                      Creator
                    </span>
                  </div>
                
              </div>
            
            <ul class="md-post__meta md-nav__list">
              <li class="md-nav__item md-nav__item--section">
                <div class="md-post__title">
                  <span class="md-ellipsis">
                    Metadata
                  </span>
                </div>
                <nav class="md-nav">
                  <ul class="md-nav__list">
                    <li class="md-nav__item">
                      <div class="md-nav__link">
                        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 19H5V8h14m-3-7v2H8V1H6v2H5c-1.11 0-2 .89-2 2v14a2 2 0 0 0 2 2h14a2 2 0 0 0 2-2V5a2 2 0 0 0-2-2h-1V1m-1 11h-5v5h5z"/></svg>
                        <time datetime="2024-09-16 00:00:00" class="md-ellipsis">September 16, 2024</time>
                      </div>
                    </li>
                    
                      <li class="md-nav__item">
                        <div class="md-nav__link">
                          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M15 13h1.5v2.82l2.44 1.41-.75 1.3L15 16.69zm4-5H5v11h4.67c-.43-.91-.67-1.93-.67-3a7 7 0 0 1 7-7c1.07 0 2.09.24 3 .67zM5 21a2 2 0 0 1-2-2V5c0-1.11.89-2 2-2h1V1h2v2h8V1h2v2h1a2 2 0 0 1 2 2v6.1c1.24 1.26 2 2.99 2 4.9a7 7 0 0 1-7 7c-1.91 0-3.64-.76-4.9-2zm11-9.85A4.85 4.85 0 0 0 11.15 16c0 2.68 2.17 4.85 4.85 4.85A4.85 4.85 0 0 0 20.85 16c0-2.68-2.17-4.85-4.85-4.85"/></svg>
                          <time datetime="2024-09-23 00:00:00" class="md-ellipsis">September 23, 2024</time>
                        </div>
                      </li>
                    
                    
                      <li class="md-nav__item">
                        <div class="md-nav__link">
                          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9 3v15h3V3zm3 2 4 13 3-1-4-13zM5 5v13h3V5zM3 19v2h18v-2z"/></svg>
                          <span class="md-ellipsis">
                            in
                            
                              <a href="../../../../category/reinforcement-learning/">Reinforcement Learning</a></span>
                        </div>
                      </li>
                    
                    
                      
                      <li class="md-nav__item">
                        <div class="md-nav__link">
                          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 20a8 8 0 0 0 8-8 8 8 0 0 0-8-8 8 8 0 0 0-8 8 8 8 0 0 0 8 8m0-18a10 10 0 0 1 10 10 10 10 0 0 1-10 10C6.47 22 2 17.5 2 12A10 10 0 0 1 12 2m.5 5v5.25l4.5 2.67-.75 1.23L11 13V7z"/></svg>
                          <span class="md-ellipsis">
                            
                              4 min read
                            
                          </span>
                        </div>
                      </li>
                    
                  </ul>
                </nav>
              </li>
            </ul>
          </nav>
          
        </div>
      </div>
    </div>
    <article class="md-content__inner md-typeset">
      
        


<h1 id="discrete-time-mean-variance-strategy-based-on-reinforcement-learning">Discrete-Time Mean-Variance Strategy Based on Reinforcement Learning</h1>
<p><strong>Key Words:</strong> </p>
<!-- more -->

<p><strong>Autor：</strong> Xiangyu Cui,∗, Xun Li, Yun Shi, Si Zhao</p>
<p>[TODO]这篇综述可以看一看
Hambly, B., Xu, R., and Yang, H. (2023). Recent advances in reinforcement learning in
finance. Mathematical Finance, 33(3):437–503.</p>
<h2 id="2">2 离散时间探索性的均值方差问题</h2>
<ul>
<li>2.1 经典离散时间均值方差问题</li>
<li>2.2 离散时间探索性的均值方差问题</li>
</ul>
<h3 id="21">2.1 经典离散时间均值方差问题</h3>
<p>首先考虑由两个资产组成的市场：</p>
<ul>
<li>无风险资产，回报率 <span class="arithmatex">\(r_f\)</span></li>
<li>风险资产，t到t+1的超额回报率 <span class="arithmatex">\(r_t\)</span>，服从均值为 <span class="arithmatex">\(a\)</span>，方差为 <span class="arithmatex">\(\sigma^2\)</span> 的正态分布。</li>
</ul>
<p><strong>假设：</strong> <span class="arithmatex">\(r_t,t=0,1,\dots,T-1\)</span>  是统计上独立的</p>
<ul>
<li>投资者进入市场的初始财富为 <span class="arithmatex">\(x_0\)</span></li>
<li>投资者希望在策略 <span class="arithmatex">\(\mathbf{u} = \{u_0, u_1,\dots,u_{T-1} \}\)</span> 下的最终财富期望值为 <span class="arithmatex">\(x_T = b\)</span></li>
</ul>
<p>简单说，投资者将会面临以下的优化问题：</p>
<div class="arithmatex">\[\begin{align*}
        \min\ &amp;\operatorname{Var}(x_T^u), \\[2mm]
        \text{s.t. }&amp;\mathbb{E}[x_T^u] = b,\\[2mm]
        &amp;x_{t+1}= r_f x_t + r_t u_t,\quad\quad t = 0,1,\dots,T-1
\end{align*}\]</div>
<p>引入 Lagrange multiplier <span class="arithmatex">\(w\)</span>，将问题转换为无约束条件问题。</p>
<div class="arithmatex">\[\begin{equation}
    \min_u\ \mathbb{E}(x_T^u)^2-b^2-2w(\mathbb{E}[x_T^u]-b) = \min_u\ \mathbb{E}(x_T^u - w)^2 - (w-b)^2.
\end{equation}\]</div>
<p>该问题存在解析解 <span class="arithmatex">\(\mathbf{u}^*=\{u_0^*,u_1^*,\dots,u_{T-1}^*\}\)</span> 依赖于 <span class="arithmatex">\(w\)</span>.
详细的推导可见 <em>(2000)Optimal dynamic portfolio selection: Multiperiod mean-variance formulation</em></p>
<h3 id="22">2.2 离散时间探索性的均值方差问题</h3>
<p>在强化学习框架中，控制过程 <span class="arithmatex">\(\mathbf{u} = \{u_t,0\le t &lt;T\}\)</span> 是随机化的，代表探索和学习，从而形成了以测度值或分布控制的过程，其密度函数表示为 <span class="arithmatex">\(\boldsymbol{\pi} = \{\pi_t,0\le t &lt; T\}\)</span>.
因此，财富的动态表示为：</p>
<div class="arithmatex">\[\begin{equation}
    x_{t+1}^\pi = r_f x_t^\pi + r_t u_t^\pi.
\end{equation}\]</div>
<p><strong>假定：</strong></p>
<ul>
<li>超额收益 <span class="arithmatex">\(r_t\)</span> 服从一个均值为 <span class="arithmatex">\(a\)</span> 和方差 <span class="arithmatex">\(\sigma^2\)</span> 的分布。</li>
<li><span class="arithmatex">\(u_t^\pi\)</span> 是一个随机控制过程，概率密度为 <span class="arithmatex">\(\pi_t\)</span>。</li>
<li><span class="arithmatex">\(r_t\)</span> 与 <span class="arithmatex">\(u_t^\pi\)</span> 是独立的。</li>
</ul>
<p>那么在周期 <span class="arithmatex">\(t\)</span> 处，<span class="arithmatex">\(r_t u_t^\pi\)</span> 的条件一阶矩和二阶矩可以表示为：</p>
<div class="arithmatex">\[\begin{align*}
    &amp;\mathbb{E}_t[r_tu_t^\pi] = \mathbb{E}_t[r_t]\mathbb{E}_t[u_t^\pi] = a\int_{\mathbb{R}}u\pi_t(u)du,\\
    &amp;\mathbb{E}_t[(r_tu_t^\pi)^2]=\mathbb{E}_t[(r_t)^2]\mathbb{E}_t[(u_t^\pi)^2]=(a^2+\sigma^2)\int_{\mathbb{R}}u^2\pi_t(u)du.
\end{align*}\]</div>
<p>随机分布控制过程 <span class="arithmatex">\(\boldsymbol{\pi} =\{\pi_t,0 \le t &lt; T\}\)</span> 建模总体水平由累积熵捕捉的探索。</p>
<div class="arithmatex">\[\begin{equation*}
    \mathcal{H}(\boldsymbol{\pi}) := -\sum_{t=0}^{T-1}\int_{\mathbb{R}}\pi_t(u)\ln\pi_t(u)du.
\end{equation*}\]</div>
<p>在离散时间市场环境下，探索性MV问题的目标函数变为：</p>
<div class="arithmatex">\[\begin{equation}
    V^\pi = \min_\pi \mathbb{E}\left[(x_T^\pi - w)^2 + \lambda \sum_{t=0}^{T-1}\int_{\mathbb{R}}\pi_t(u)\ln\pi_t(u)du\right] - (w-b)^2.
\end{equation}\]</div>
<details class="原先的目标函数">
<summary>原先的目标函数</summary>
<div class="arithmatex">\[\begin{equation*}
    \min_u\ \mathbb{E}(x_T^u - w)^2 - (w-b)^2.
\end{equation*}\]</div>
</details>
<ul>
<li><span class="arithmatex">\(\lambda :\)</span> temperature parameter measures the trade-off between exploitation and exploration in this MV problem.</li>
</ul>
<p>问题(3)中理论的最优反馈控制和相应的最优值函数如下：</p>
<p>定义在策略 <span class="arithmatex">\(\boldsymbol{\pi}\)</span> 下的值函数 <span class="arithmatex">\(J(t,x;w)\)</span>:</p>
<div class="arithmatex">\[\begin{equation*}
    J(t,x;w) = \mathbb{E}\left[(x_T^\pi - w)^2 + \lambda \sum_{s=t}^{T-1}\int_{\mathbb{R}}\pi_t(u)\ln\pi_t(u)du \mid x_t^\pi = x \right] - (w-b)^2
\end{equation*}\]</div>
<p><span class="arithmatex">\(J^*(t,x;w)\)</span> 称为问题(3)的最优值函数：</p>
<div class="arithmatex">\[\begin{align*}
    &amp;J^*(t,x;w) = \min_{\pi_{t}, \ldots, \pi_{T-1}}\mathbb{E}\left[(x_T^\pi - w)^2 + \lambda \sum_{s=t}^{T-1}\int_{\mathbb{R}}\pi_t(u)\ln\pi_t(u)du \mid x_t^\pi = x \right] - (w-b)^2,\\
    &amp;J^*(T,x;w) = (x-w)^2 - (w-b)^2.
\end{align*}\]</div>
<hr />
<p><strong>Theorem 1.</strong> At period t, the optimal value function is given by</p>
<div class="arithmatex">\[\begin{equation}
    \begin{split}
        &amp;J^*(t,x;w) \\
        =&amp;\left(\frac{\sigma^2 r_f^2}{a^2+\sigma^2}\right)^{T-t}(x-\rho_t w)^2 + \frac{\lambda}{2}(T-t)\ln\left(\frac{a^2 + \sigma^2}{\pi \lambda}\right) \\
        &amp;+ \frac{\lambda}{2}\sum_{i=t+1}^{T}(T-i)\ln\left(\frac{\sigma^2 r_f^2}{a^2 + \sigma^2}\right) - (w-b)^2.
    \end{split}
\end{equation}\]</div>
<p>where <span class="arithmatex">\(\rho_t = (r_f^{-1})^{T-t}\)</span>. Moreover, the optimal feedback control is Gaussian, with its density function given by</p>
<div class="arithmatex">\[\begin{equation}
    \pi^*(u;t,x,w) = \mathcal{N}\left(u \,\Bigg| \, - \frac{ar_f(x-\rho_t w)}{a^2 + \sigma^2}, \frac{\lambda}{2(a^2 + \sigma^2)}\left( \frac{a^2 + \sigma^2}{\sigma^2 r_f^2} \right)^{T-t-1}\right) .
\end{equation}\]</div>
<hr />
<p>[NOTE:超额收益 <span class="arithmatex">\(r_t\)</span> 服从一个均值为 <span class="arithmatex">\(a\)</span> 和方差 <span class="arithmatex">\(\sigma^2\)</span> 的分布。]</p>
<details class="定理1有两点需要注意">
<summary>定理1有两点需要注意</summary>
<ol>
<li>方差项衡量最优的高斯策略在 <span class="arithmatex">\(t\)</span> 时刻的的探索程度，意味着探索会随着时间衰减。agent 初始会最大程度的进行探索，然后探索程度会随时间逐渐衰减。
随着时间接近到期日，利用会主导探索并且会变得越来越重要，这是因为有一个将对投资者的行动进行评估的 deadline T。</li>
<li>最优高斯策略的均值与探索权重 <span class="arithmatex">\(\lambda\)</span> 无关，方差与状态 <span class="arithmatex">\(x\)</span> 无关。完美的将<strong>利用</strong>和<strong>探索</strong>进行了分离，因为前者由均值捕捉，后者由方差捕捉。</li>
</ol>
</details>
<h2 id="3">3. 离散时间算法</h2>
<p>上述内容证明并给出了离散时间探索性均值-方差问题的最优解。接下来设计对应的RL算法来
直接学习求解和输出投资组合分配策略。接下来要做的内容有两部分：</p>
<ul>
<li>1.策略提升定理和策略收敛</li>
<li>2.一个自校正方案来学习真正的拉格朗日乘子 <span class="arithmatex">\(w\)</span> </li>
</ul>
<details class="注意">
<summary>注意</summary>
<p>该RL算法跳过了模型参数的估计，如超额回报 <span class="arithmatex">\(r_t\)</span> 的均值和方差，这些参数难以准确估计。</p>
</details>
<h3 id="31">3.1 策略提升定理和策略收敛</h3>
<details class="为什么需要策略提升定理和策略收敛">
<summary>为什么需要策略提升定理和策略收敛</summary>
<p>策略改进方案可以向正确的方向更新当前策略，以改进 value function 。
<strong>策略提升定理</strong>会保证迭代后的 value function 是非增的（在最小化的问题的情况下）。
<strong>策略收敛</strong>保证会最终收敛到 optimal value function 。</p>
</details>
<hr />
<p><strong>Theorem 2.</strong> Suppose <span class="arithmatex">\(w\in \mathbb{R}\)</span> is fixed and <span class="arithmatex">\(\boldsymbol{\pi}^0\)</span> is an arbitrarily given admissible feedback
control policy, subjecting to</p>
<div class="arithmatex">\[ 
\pi_t^0(u;x,w) = \mathcal{N}\left( u \mid K(x-\rho_t w), \lambda BC^{T-t-1} \right)
\]</div>
<p>Then we can calculate <span class="arithmatex">\(J^{\pi^0}(t,x;w)\)</span>, where <span class="arithmatex">\(\rho_t = r_f^{-(T-t)}\)</span>, <span class="arithmatex">\(A=r_f^2 + (a^2 + \sigma^2)K^2 + 2r_f a K\)</span> and 
<span class="arithmatex">\(f(t) = \frac{\lambda B (a^2 + \sigma^2)[1-(CA)^{T-t}]}{1-CA} - \frac{\lambda}{2}\ln (2\pi \lambda B)(T-t) - \frac{\lambda}{2}(T-t)-\frac{\lambda}{2}\ln C\sum_{i=0}^{T-t-1}i-(w-b)^2\)</span> 
is a smooth function that only depends on T</p>
<div class="arithmatex">\[\begin{equation*}
    \begin{split}
        &amp;J^{\pi^0}(t,x;w) \\
        = &amp; A^{T-t}(x-\rho_t w)^2 + \frac{\lambda B (a^2 + \sigma^2)[1-(CA)^{T-t}]}{1-CA} - \frac{\lambda}{2}\ln (2\pi \lambda B)(T-t)\\
        &amp; - \frac{\lambda}{2}(T-t)-\frac{\lambda}{2}\ln C\sum_{i=0}^{T-t-1}i-(w-b)^2 \\
        = &amp; A^{T-t}(x-\rho_t w)^2 + f(t)
    \end{split}
\end{equation*}\]</div>
<p>Using the condition 
<span class="arithmatex">\(\pi_{t}^{k+1}(u ; x, w)=\arg \min _{\pi_{t}^{k}(u)} J^{\pi^{k}}(t, x ; w)\)</span>
to update the feedback policy and making this iteration for k times, we can get 
<span class="arithmatex">\(\pi_{t}^{k}(u ; x, w)\)</span> 
and the corresponding value function
<span class="arithmatex">\(J^{\pi^{k}}(t, x ; w):\)</span></p>
<div class="arithmatex">\[\begin{equation}
    \pi_{t}^{k}(u ; x, w)=\mathcal{N}\left(u \left\lvert\,-\frac{a r_{f}\left(x-\rho_{t} w\right)}{a^{2}+\sigma^{2}}\right., \frac{\lambda}{2\left(a^{2}+\sigma^{2}\right) A^{T-t-k}}\left(\frac{a^{2}+\sigma^{2}}{\sigma^{2} r_{f}^{2}}\right)^{k-1} \right),
\end{equation}\]</div>
<hr />







  
  




  



      
    </article>
  </div>

          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2016 - 2024 Zhengzihao
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
      <div class="md-progress" data-md-component="progress" role="progressbar"></div>
    
    
    <script id="__config" type="application/json">{"base": "../../../../..", "features": ["navigation.indexes", "navigation.instant", "navigation.instant.prefetch", "navigation.instant.progress", "navigation.expand"], "search": "../../../../../assets/javascripts/workers/search.07f07601.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../../../../assets/javascripts/bundle.56dfad97.min.js"></script>
      
        <script src="../../../../../javascripts/mathjax.js"></script>
      
        <script src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>